# Private AI Chat Assistant: Pure Browser, Zero Backend

## Download and run local LLMs within your browser.

Live site: https://wollama.vercel.app

## Running locally

1. Install dependencies
```
npm install
```

2. Start web app
```
npm run dev
```

3. Navigate to http://localhost:5173/ 

## Credits
- [Wllama](https://github.com/ngxson/wllama)
- SmolLm - [HuggingFace](https://huggingface.co/HuggingFaceTB)
- Llama 3.2 - [Meta](https://www.llama.com/)
- Deepseek R1 - [DeepSeek](https://www.deepseek.com/)

